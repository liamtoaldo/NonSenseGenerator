@startuml
title Internal Sequence Diagram for Toxicity Analysis
actor User
participant "ApplicationRestController"
participant "ApplicationController"
participant "Sentence"
participant "ToxicityAnalyzer"
participant "APIClient"
participant "Google API"
participant "ModerationResult"
participant "ModerationCategory"

User -> ApplicationRestController: getSentenceToxicity(String)
activate ApplicationRestController
note over ApplicationController, Sentence: Convert the input string to a Sentence object
ApplicationRestController -> ApplicationController: convertStringToSentence(String)
activate ApplicationController
ApplicationController -> Sentence: new Sentence(String)
activate Sentence
Sentence --> ApplicationController: sentence
deactivate Sentence
ApplicationController -> ApplicationRestController: sentence
ApplicationRestController -> ApplicationController: getSentenceToxicity(Sentence)
alt sentence is not null or empty
    ApplicationController -> ToxicityAnalyzer: analyzeToxicity(Sentence)
    activate ToxicityAnalyzer

    ToxicityAnalyzer -> APIClient: post(String, Map<String, String>, JSONObject)
    activate APIClient
    APIClient -> "Google API": POST /v1/documents:moderateText
    activate "Google API"
    "Google API" --> APIClient: JSON Response
    deactivate "Google API"
    alt API response is valid and contains categories
        APIClient -> ToxicityAnalyzer: response
        deactivate APIClient
        ToxicityAnalyzer -> ModerationResult: new ModerationResult()
        note over ToxicityAnalyzer: Populate ModerationResult with categories from API response
        activate ModerationResult
        loop for each category in response
            ToxicityAnalyzer -> ModerationCategory: new ModerationCategory(String, double)
            activate ModerationCategory
            alt category name is not null and not empty
                alt category confidence is greater than or equal to 0 and less than or equal to 1
                    ModerationCategory --> ToxicityAnalyzer: category
                else category confidence is invalid
                    ModerationCategory --> ToxicityAnalyzer: throw IllegalArgumentException("Confidence must be between 0 and 1")
                end
            else category name is null or empty
                ModerationCategory --> ToxicityAnalyzer: throw IllegalArgumentException("Name cannot be null or empty")
                deactivate ModerationCategory
            end
            deactivate ModerationCategory
        end
        ModerationResult --> ToxicityAnalyzer: result
        deactivate ModerationResult
        ToxicityAnalyzer --> ApplicationController: result
        ApplicationController --> ApplicationRestController: result
    else API response is invalid or categories not found
        ToxicityAnalyzer --> ApplicationController: throw RuntimeException("Failed to get a valid response from the API")
        deactivate ToxicityAnalyzer
    end
else sentence is null or empty
    ApplicationController --> ApplicationRestController: throw IllegalArgumentException("Sentence cannot be null or empty")
    deactivate ApplicationController
end

alt Error in analysis
    ApplicationRestController --> User: error
else moderation result is valid
    ApplicationRestController --> User: moderation result
end
deactivate ApplicationRestController
@enduml